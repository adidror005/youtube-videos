{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adidror005/youtube-videos/blob/main/old_videos/LLAMA_3_Fine_Tuning_for_Sequence_Classification_Actual_Video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLAMA3 Fine-tuning for text classification using QLORA\n",
        "\n",
        "\n",
        "### Requirements:\n",
        "* A GPU with enough memory!\n",
        "\n",
        "### Installs\n",
        "* They suggest using latest version of transformers\n",
        "* Must restart after install because the accelerate package used in the hugging face trainer requires it."
      ],
      "metadata": {
        "id": "IqufrL0vwDod"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzSvk9-psdeH",
        "outputId": "17fc46b6-ccb9-410e-81d3-84ecd9f65f01",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.2.2\n",
            "  Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch==2.2.2)\n",
            "  Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.5.82)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.8)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (5.29.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n",
            "Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl (755.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.6/755.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m127.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# Install Pytorch\n",
        "%pip install \"torch==2.2.2\" tensorboard\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "%pip install  --upgrade \"transformers==4.40.0\" \"datasets==2.18.0\" \"accelerate==0.29.3\" \"evaluate==0.4.1\" \"bitsandbytes==0.43.1\" \"huggingface_hub==0.22.2\" \"trl==0.8.6\" \"peft==0.10.0\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Big Picture Overview of Parameter Efficient Fine Tuning Methods like LoRA and QLoRA Fine Tuning for Sequence Classification\n",
        "\n",
        "**The Essence of Fine-tuning**\n",
        "- LLMs are pre-trained on vast amounts of data for broad language understanding.\n",
        "- Fine-tuning is crucial for specializing in specific domains or tasks, involving adjustments with smaller, relevant datasets.\n",
        "\n",
        "**Model Fine-tuning with PEFT: Exploring LoRA and QLoRA**\n",
        "- Traditional fine-tuning is resource-intensive; PEFT (Parameter Efficient Fine-tuning) makes the process faster and less demanding.\n",
        "- Focus on two PEFT methods: LoRA and QLoRA.\n",
        "\n",
        "**The Power of PEFT**\n",
        "- PEFT modifies only a subset of the LLM's parameters, enhancing speed and reducing memory demands, making it suitable for less powerful devices.\n",
        "\n",
        "**LoRA: Efficiency through Adapters**\n",
        "- **Low-Rank Adaptation (LoRA):** Injects small trainable adapters into the pre-trained model.\n",
        "- **Equation:** For a weight matrix $W$, LoRA approximates $W = W_0 + BA$, where $W_0$ is the original weight matrix, and $BA$ represents the low-rank modification through trainable matrices $B$ and $A$.\n",
        "- Adapters learn task nuances while keeping the majority of the LLM unchanged, minimizing overhead.\n",
        "\n",
        "**QLoRA: Compression and Speed**\n",
        "- **Quantized LoRA (QLoRA):** Extends LoRA by quantizing the model’s weights, further reducing size and enhancing speed.\n",
        "- **Innovations in QLoRA:**\n",
        "  1. **4-bit Quantization:** Uses a 4-bit data type, NormalFloat (NF4), for optimal weight quantization, drastically reducing memory usage.\n",
        "  2. **Low-Rank Adapters:** Fine-tuned with 16-bit precision to effectively capture task-specific nuances.\n",
        "  3. **Double Quantization:** Reduces quantization constants from 32-bit to 8-bit, saving additional memory without accuracy loss.\n",
        "  4. **Paged Optimizers:** Manages memory efficiently during training, optimizing for large tasks.\n",
        "\n",
        "**Why PEFT Matters**\n",
        "- **Rapid Learning:** Speeds up model adaptation.\n",
        "- **Smaller Footprint:** Eases deployment with reduced model size.\n",
        "- **Edge-Friendly:** Fits better on devices with limited resources, enhancing accessibility.\n",
        "\n",
        "**Conclusion**\n",
        "- PEFT methods like LoRA and QLoRA revolutionize LLM fine-tuning by focusing on efficiency, facilitating faster adaptability, smaller models, and broader device compatibility.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jIS0yOVNRHcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning for Sentiment Analysis Classification:\n",
        "\n",
        "\n",
        "#### 1. Text Generation with Sentiment Label as part of text\n",
        "- **Approach**: Train the model to generate text that naturally appends the sentiment label at the end.\n",
        "- **Input**: \"TSLA slashes model Y prices ======\"\n",
        "- **Output**: \"TSLA slashes model Y prices ====== Bearish\"\n",
        "- **Use Case**: This method is useful for applications requiring continuous text output that includes embedded sentiment analysis, such as interactive chatbots or automated content creation tools.\n",
        "\n",
        "\n",
        "#### 2. Sequence Classification Head\n",
        "- **Approach**: Add a sequence classification head (linear layer) on top of the LLaMa Model transformer. This setup is similar to GPT-2 and focuses on classifying the sentiment based on the last relevant token in the sequence.\n",
        "    - **Token Positioning**:\n",
        "        - **With pad_token_id**: The model identifies and ignores padding tokens, using the last non-padding token for classification.\n",
        "        - **Without pad_token_id**: It defaults to the last token in each sequence.\n",
        "        - **inputs_embeds**: If embeddings are directly passed (without input_ids), the model cannot identify padding tokens and takes the last embedding in each sequence as the input for classification.\n",
        "- **Input**: Specific sentences (e.g., \"TSLA slashes Model Y prices\").\n",
        "- **Output**: Direct sentiment classification (e.g., \"Bearish\").\n",
        "- **Training Objective**: Minimize cross-entropy loss between the predicted and the actual sentiment labels.\n",
        "\n",
        "https://huggingface.co/docs/transformers/main/en/model_doc/llama\n",
        "\n",
        "### Peft Configs\n",
        "* Bits and bytes config for quantization\n",
        "* Lora config for lora\n",
        "\n",
        "### Going to use Hugginface Transformers trainer class: Main componenents\n",
        "* Hugging face dataset (for train + eval)\n",
        "* Data collater\n",
        "* Compute Metrics\n",
        "* Class weights since we use custom trainer and also custom weighted loss..\n",
        "* trainingArgs: like # epochs, learning rate, weight decay etc..\n",
        "\n"
      ],
      "metadata": {
        "id": "QAs9tbj8tiBZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCNt55YNyA3d"
      },
      "source": [
        "### Login to huggingface hub to put your LLama token so we can access Llama 3 7B Param Pre-trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8157Tsw3Vo3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Imports"
      ],
      "metadata": {
        "id": "Z_5AVUyey1io"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5NPLc7isjdM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import functools\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import evaluate\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, confusion_matrix, classification_report, balanced_accuracy_score, accuracy_score\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Youtube Video Describing How to get Dataset\n",
        "* Only really need first 3 mins of video"
      ],
      "metadata": {
        "id": "A5NaDTjvbO5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('ascf3y7zSaY')"
      ],
      "metadata": {
        "id": "aO711xGybRai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPHo5hnA1Fsq"
      },
      "source": [
        "#### Load TSLA sentiment analysis dataset\n",
        "* Derived from Alpha vantage text data...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFIRWaIH1SHb"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"tsla_sentiment.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Add also a numeric 0,1,2 version of label since we will need it later for fine tuning. We can save it in 'target'"
      ],
      "metadata": {
        "id": "tC0ls6RVlzHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['label']=df['label'].astype('category')\n",
        "df['target']=df['label'].cat.codes\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "kHiBd07ikx0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Suppose you want to decode later"
      ],
      "metadata": {
        "id": "OnaLFMhfk1u_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'].cat.categories"
      ],
      "metadata": {
        "id": "r0eMOMiky_8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "category_map = {code: category for code, category in enumerate(df['label'].cat.categories)}\n",
        "category_map"
      ],
      "metadata": {
        "id": "oZEezWPak0Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKz778AH1zSZ"
      },
      "source": [
        "### Split into train/val/test for later comparison.\n",
        "* For simplicity we split based on time.\n",
        "  - First 60% train\n",
        "  - Next 20% val\n",
        "  - Next 20% test\n",
        "* This can be problematic a bit since class balance changes over time and some articles on boundries between train/val or val/test have some overlap, but completely beats bias of stratified sample usually used since some articles are literally on same thing, but maybe different sources.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6otP7Zi1z20"
      },
      "outputs": [],
      "source": [
        "train_end_point = int(df.shape[0]*0.6)\n",
        "val_end_point = int(df.shape[0]*0.8)\n",
        "df_train = df.iloc[:train_end_point,:]\n",
        "df_val = df.iloc[train_end_point:val_end_point,:]\n",
        "df_test = df.iloc[val_end_point:,:]\n",
        "print(df_train.shape, df_test.shape, df_val.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert from Pandas DataFrame to Hugging Face Dataset\n",
        "* Also let's shuffle the training set.\n",
        "* We put the components train,val,test into a DatasetDict so we can access them later with HF trainer.\n",
        "* Later we will add a tokenized dataset\n"
      ],
      "metadata": {
        "id": "nv3ToinDzIwE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1g5EdzTN21Tq"
      },
      "outputs": [],
      "source": [
        "# Converting pandas DataFrames into Hugging Face Dataset objects:\n",
        "dataset_train = Dataset.from_pandas(df_train.drop('label',axis=1))\n",
        "dataset_val = Dataset.from_pandas(df_val.drop('label',axis=1))\n",
        "dataset_test = Dataset.from_pandas(df_test.drop('label',axis=1))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the training dataset\n",
        "dataset_train_shuffled = dataset_train.shuffle(seed=42)  # Using a seed for reproducibility\n"
      ],
      "metadata": {
        "id": "EjSFUqr4zbG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine them into a single DatasetDict\n",
        "dataset = DatasetDict({\n",
        "    'train': dataset_train_shuffled,\n",
        "    'val': dataset_val,\n",
        "    'test': dataset_test\n",
        "})\n",
        "dataset"
      ],
      "metadata": {
        "id": "xREu-St-zeGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train']"
      ],
      "metadata": {
        "id": "sTP9exNizh1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Since our classes are not balanced let's calculate class weights based on inverse value counts\n",
        "* Convert to pytorch tensor since we will need it"
      ],
      "metadata": {
        "id": "b6mX_Hfe0hei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.target.value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "Z6z0M7tf0g3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights=(1/df_train.target.value_counts(normalize=True).sort_index()).tolist()\n",
        "class_weights=torch.tensor(class_weights)\n",
        "class_weights=class_weights/class_weights.sum()\n",
        "class_weights\n"
      ],
      "metadata": {
        "id": "e6Nvgfy-zsyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load LLama model with 4 bit quantization as specified in bits and bytes and prepare model for peft training\n",
        "\n",
        "### Model Name"
      ],
      "metadata": {
        "id": "QolyR2Cd2Bsg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXkyNcgt2fet"
      },
      "outputs": [],
      "source": [
        "model_name = \"meta-llama/Meta-Llama-3-8B\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Quantization Config (for QLORA)"
      ],
      "metadata": {
        "id": "tzRyVNmN2jlO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtR7MXs43GJf"
      },
      "outputs": [],
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit = True, # enable 4-bit quantization\n",
        "    bnb_4bit_quant_type = 'nf4', # information theoretically optimal dtype for normally distributed weights\n",
        "    bnb_4bit_use_double_quant = True, # quantize quantized weights //insert xzibit meme\n",
        "    bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lora Config"
      ],
      "metadata": {
        "id": "AxRLidIwS4Xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r = 16, # the dimension of the low-rank matrices\n",
        "    lora_alpha = 8, # scaling factor for LoRA activations vs pre-trained weight activations\n",
        "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
        "    lora_dropout = 0.05, # dropout probability of the LoRA layers\n",
        "    bias = 'none', # wether to train bias weights, set to 'none' for attention layers\n",
        "    task_type = 'SEQ_CLS'\n",
        ")"
      ],
      "metadata": {
        "id": "EG950ljoS3RM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load model\n",
        "* AutomodelForSequenceClassification\n",
        "* Num Labels is # of classes\n"
      ],
      "metadata": {
        "id": "Brl04t2KS69t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJtZAdKp4WdT"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    num_labels=3\n",
        ")\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* prepare_model_for_kbit_training() function to preprocess the quantized model for training."
      ],
      "metadata": {
        "id": "prFT0qY0mVkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = prepare_model_for_kbit_training(model)\n",
        "model"
      ],
      "metadata": {
        "id": "-NcEtG0jmTqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* get_peft_model prepares a model for training with a PEFT method such as LoRA by wrapping the base model and PEFT configuration with get_peft_model"
      ],
      "metadata": {
        "id": "25JDWS0Hmb0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, lora_config)\n",
        "model"
      ],
      "metadata": {
        "id": "zIXKJgTfmU-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the tokenizer\n",
        "\n",
        "#### Since LLAMA3 pre-training doesn't have EOS token\n",
        "* Set the pad_token_id to eos_token_id\n",
        "* Set pad token ot eos_token"
      ],
      "metadata": {
        "id": "4j9Ubd2DVAOW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzS5OhVO8Tuo"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
        "\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Update some model configs\n",
        "* Must use .cache = False as below or it crashes from my experience"
      ],
      "metadata": {
        "id": "akDra1649hcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1"
      ],
      "metadata": {
        "id": "XBFCNrrE9hAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loop through dataset to measure performance before training/fitting the model\n",
        "* Use a batch size 32 to kinda vectorize and to avoid memory errors."
      ],
      "metadata": {
        "id": "eWoLZTYf3lCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = df_test.summary.tolist()\n",
        "sentences[0:2]"
      ],
      "metadata": {
        "id": "barsbGNJ08JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert summaries to a list\n",
        "sentences = df_test.summary.tolist()\n",
        "\n",
        "# Define the batch size\n",
        "batch_size = 32  # You can adjust this based on your system's memory capacity\n",
        "\n",
        "# Initialize an empty list to store the model outputs\n",
        "all_outputs = []\n",
        "\n",
        "# Process the sentences in batches\n",
        "for i in range(0, len(sentences), batch_size):\n",
        "    # Get the batch of sentences\n",
        "    batch_sentences = sentences[i:i + batch_size]\n",
        "\n",
        "    # Tokenize the batch\n",
        "    inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # Move tensors to the device where the model is (e.g., GPU or CPU)\n",
        "    inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') for k, v in inputs.items()}\n",
        "\n",
        "    # Perform inference and store the logits\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        all_outputs.append(outputs['logits'])\n",
        "\n"
      ],
      "metadata": {
        "id": "1GECdZk_Iso0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Concatenate all outputs into a single tensor"
      ],
      "metadata": {
        "id": "LHBh3ML64rc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_outputs = torch.cat(all_outputs, dim=0)\n",
        "final_outputs"
      ],
      "metadata": {
        "id": "I0t0AWmd4kcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* argmax to get class prediction"
      ],
      "metadata": {
        "id": "_vfhekJH4ucy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_outputs.argmax(axis=1)"
      ],
      "metadata": {
        "id": "K5Suw61G4qOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Move to CPU so we can use numpy and set prediction colum to it"
      ],
      "metadata": {
        "id": "mtsgVzj3JMk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['predictions']=final_outputs.argmax(axis=1).cpu().numpy()\n",
        "df_test['predictions']"
      ],
      "metadata": {
        "id": "XcW9bs5K5Upf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['predictions'].value_counts()"
      ],
      "metadata": {
        "id": "w-8_az8IXKyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Use category map to get back category names"
      ],
      "metadata": {
        "id": "C4pmQIpc6RNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['predictions']=df_test['predictions'].apply(lambda l:category_map[l])\n",
        "df_test['predictions']"
      ],
      "metadata": {
        "id": "s2mhkH5r6TgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze performance as in intro notebook"
      ],
      "metadata": {
        "id": "fPokM-op4ZZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_performance_metrics(df_test):\n",
        "  y_test = df_test.label\n",
        "  y_pred = df_test.predictions\n",
        "\n",
        "  print(\"Confusion Matrix:\")\n",
        "  print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "  print(\"\\nClassification Report:\")\n",
        "  print(classification_report(y_test, y_pred))\n",
        "\n",
        "  print(\"Balanced Accuracy Score:\", balanced_accuracy_score(y_test, y_pred))\n",
        "  print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "ZJ3eFsPz4Hd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_performance_metrics(df_test)"
      ],
      "metadata": {
        "id": "05xkpOOS03Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer Components\n",
        "* model\n",
        "* tokenizer\n",
        "* training arguments\n",
        "* train dataset\n",
        "* eval dataset\n",
        "* Data Collater\n",
        "* Compute Metrics\n",
        "* class_weights: In our case since we are using a custom trainer so we can use a weighted loss we will subclass trainer and define the custom loss."
      ],
      "metadata": {
        "id": "0aM2eCK47kf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create LLAMA tokenized dataset which will house our train/val parts during the training process but after applying tokenization"
      ],
      "metadata": {
        "id": "TQCINMPIVgvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 512\n",
        "col_to_delete = ['time_published', 'summary']\n",
        "\n",
        "def llama_preprocessing_function(examples):\n",
        "    return tokenizer(examples['summary'], truncation=True, max_length=MAX_LEN)\n",
        "\n",
        "tokenized_datasets = dataset.map(llama_preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"target\", \"label\")\n",
        "tokenized_datasets.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "MJ8t0ZtVVfPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Collator\n",
        "A **data collator** prepares batches of data for training or inference in machine learning, ensuring uniform formatting and adherence to model input requirements. This is especially crucial for variable-sized inputs like text sequences.\n",
        "\n",
        "### Functions of Data Collator\n",
        "\n",
        "1. **Padding:** Uniformly pads sequences to the length of the longest sequence using a special token, allowing simultaneous batch processing.\n",
        "2. **Batching:** Groups individual data points into batches for efficient processing.\n",
        "3. **Handling Special Tokens:** Adds necessary special tokens to sequences.\n",
        "4. **Converting to Tensor:** Transforms data into tensors, the required format for machine learning frameworks.\n",
        "\n",
        "### `DataCollatorWithPadding`\n",
        "\n",
        "The `DataCollatorWithPadding` specifically manages padding, using a tokenizer to ensure that all sequences are padded to the same length for consistent model input.\n",
        "\n",
        "- **Syntax:** `collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)`\n",
        "- **Purpose:** Automatically pads text data to the longest sequence in a batch, crucial for models like BERT or GPT.\n",
        "- **Tokenizer:** Uses the provided `tokenizer` for sequence processing, respecting model-specific vocabulary and formatting rules.\n",
        "\n",
        "This collator is commonly used with libraries like Hugging Face's Transformers, facilitating data preprocessing for various NLP models.\n"
      ],
      "metadata": {
        "id": "LFMAsvJFVlMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "XfpRu7l5Vjx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# define which metrics to compute for evaluation\n",
        "* We will use balanced accuracy and accuracy for simplicity"
      ],
      "metadata": {
        "id": "KHh8YAiqVu06"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3fjS8YO4do1"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {'balanced_accuracy' : balanced_accuracy_score(predictions, labels),'accuracy':accuracy_score(predictions,labels)}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define custom trainer with classweights\n",
        "* We will have a custom loss function that deals with the class weights and have class weights as additional argument in constructor"
      ],
      "metadata": {
        "id": "7IKaB5d6Wbni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, *args, class_weights=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        # Ensure label_weights is a tensor\n",
        "        if class_weights is not None:\n",
        "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
        "        else:\n",
        "            self.class_weights = None\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        # Extract labels and convert them to long type for cross_entropy\n",
        "        labels = inputs.pop(\"labels\").long()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Extract logits assuming they are directly outputted by the model\n",
        "        logits = outputs.get('logits')\n",
        "\n",
        "        # Compute custom loss with class weights for imbalanced data handling\n",
        "        if self.class_weights is not None:\n",
        "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
        "        else:\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ],
      "metadata": {
        "id": "wc4zAX1iXvDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# define training args"
      ],
      "metadata": {
        "id": "nfFh6JMw8y7E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AET29lE9qqw"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir = 'sentiment_classification',\n",
        "    learning_rate = 1e-4,\n",
        "    per_device_train_batch_size = 8,\n",
        "    per_device_eval_batch_size = 8,\n",
        "    num_train_epochs = 2,\n",
        "    weight_decay = 0.01,\n",
        "    evaluation_strategy = 'epoch',\n",
        "    save_strategy = 'epoch',\n",
        "    load_best_model_at_end = True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define custom trainer"
      ],
      "metadata": {
        "id": "DyDI7Wm89-0p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGHhP9R09rUR"
      },
      "outputs": [],
      "source": [
        "trainer = CustomTrainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = tokenized_datasets['train'],\n",
        "    eval_dataset = tokenized_datasets['val'],\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator = collate_fn,\n",
        "    compute_metrics = compute_metrics,\n",
        "    class_weights=class_weights,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* https://huggingface.co/docs/transformers/en/training"
      ],
      "metadata": {
        "id": "HCN4zFN0fRpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run trainer!"
      ],
      "metadata": {
        "id": "9tIQ3lxk-BLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_result = trainer.train()"
      ],
      "metadata": {
        "id": "qLXOgM0FkaKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Let's check the results\n",
        "* I wrapped in a function"
      ],
      "metadata": {
        "id": "dSTf1TMf8NpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(model,df_test):\n",
        "\n",
        "\n",
        "  # Convert summaries to a list\n",
        "  sentences = df_test.summary.tolist()\n",
        "\n",
        "  # Define the batch size\n",
        "  batch_size = 32  # You can adjust this based on your system's memory capacity\n",
        "\n",
        "  # Initialize an empty list to store the model outputs\n",
        "  all_outputs = []\n",
        "\n",
        "  # Process the sentences in batches\n",
        "  for i in range(0, len(sentences), batch_size):\n",
        "      # Get the batch of sentences\n",
        "      batch_sentences = sentences[i:i + batch_size]\n",
        "\n",
        "      # Tokenize the batch\n",
        "      inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "      # Move tensors to the device where the model is (e.g., GPU or CPU)\n",
        "      inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') for k, v in inputs.items()}\n",
        "\n",
        "      # Perform inference and store the logits\n",
        "      with torch.no_grad():\n",
        "          outputs = model(**inputs)\n",
        "          all_outputs.append(outputs['logits'])\n",
        "  final_outputs = torch.cat(all_outputs, dim=0)\n",
        "  df_test['predictions']=final_outputs.argmax(axis=1).cpu().numpy()\n",
        "  df_test['predictions']=df_test['predictions'].apply(lambda l:category_map[l])\n",
        "\n",
        "\n",
        "make_predictions(model,df_test)"
      ],
      "metadata": {
        "id": "uxIXBpIHeWKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(model,df_test):\n",
        "\n",
        "\n",
        "  # Convert summaries to a list\n",
        "  sentences = df_test.summary.tolist()\n",
        "\n",
        "  # Define the batch size\n",
        "  batch_size = 32  # You can adjust this based on your system's memory capacity\n",
        "\n",
        "  # Initialize an empty list to store the model outputs\n",
        "  all_outputs = []\n",
        "\n",
        "  # Process the sentences in batches\n",
        "  for i in range(0, len(sentences), batch_size):\n",
        "      # Get the batch of sentences\n",
        "      batch_sentences = sentences[i:i + batch_size]\n",
        "\n",
        "      # Tokenize the batch\n",
        "      inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "      # Move tensors to the device where the model is (e.g., GPU or CPU)\n",
        "      inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') for k, v in inputs.items()}\n",
        "\n",
        "      # Perform inference and store the logits\n",
        "      with torch.no_grad():\n",
        "          outputs = model(**inputs)\n",
        "          all_outputs.append(outputs['logits'])\n",
        "  final_outputs = torch.cat(all_outputs, dim=0)\n",
        "  df_test['predictions']=final_outputs.argmax(axis=1).cpu().numpy()\n",
        "  df_test['predictions']=df_test['predictions'].apply(lambda l:category_map[l])\n",
        "\n",
        "\n",
        "make_predictions(model,df_test)"
      ],
      "metadata": {
        "id": "5OGdmf228OJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_performance_metrics(df_test)"
      ],
      "metadata": {
        "id": "hrPwJ-RSsTXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the model trainer state and model adapters"
      ],
      "metadata": {
        "id": "eZTHqTuPir_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = train_result.metrics\n",
        "max_train_samples = len(dataset_train)\n",
        "metrics[\"train_samples\"] = min(max_train_samples, len(dataset_train))\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()"
      ],
      "metadata": {
        "id": "KnCfi0Z3W567"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Saving the adapter model\n",
        "* Note this doesn't save the entire model. It only saves the adapters."
      ],
      "metadata": {
        "id": "CArcZkf509Xq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"saved_model\")"
      ],
      "metadata": {
        "id": "AyDo4GzKaQTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference from Saved Model"
      ],
      "metadata": {
        "id": "E4JoRFNQbjxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "5JLs0pfabl48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r sentiment_classification /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "x7HaMw0xulhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r saved_model /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "SU3i00QSuzDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "60My1GU6u-H2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "authorship_tag": "ABX9TyNciKyK4tW1BaBSbiXC+oBh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}